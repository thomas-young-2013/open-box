# Transfer Learning

When performing BBO, users often run tasks that are similar to
previous ones. This fact can be used to speed up the current task.
Compared with Vizier, which only provides limited transfer learning
functionality for single-objective BBO problems, OpenBox employs
a general transfer learning framework with the following
advantages:

1) support for the generalized black-box optimization
problems

2) compatibility with most BO methods.
OpenBox takes as input observations from ğ¾ + 1 tasks: ğ·1, ...,
ğ·ğ¾ for ğ¾ previous tasks and ğ·ğ‘‡ for the current task. Each ğ·ğ‘– = {(ğ’™ğ‘–ğ‘—, ğ’šğ‘–ğ‘—)}ğ‘›ğ‘–
ğ‘—=1, ğ‘– = 1, ...,ğ¾, includes a set of observations. Note that,
ğ’š is an array, including multiple objectives for configuration ğ’™.
For multi-objective problems with ğ‘ objectives, we propose to
transfer the knowledge about ğ‘ objectives individually. Thus, the
transfer learning of multiple objectives is turned into ğ‘ single-objective
transfer learning processes. For each dimension of the
objectives, we take the following transfer-learning technique. 1)
We first train a surrogate model ğ‘€ğ‘– on ğ·ğ‘– for the ğ‘–ğ‘¡â„ prior task
and ğ‘€ğ‘‡ on ğ·ğ‘‡ ; based on ğ‘€1:ğ¾ and ğ‘€ğ‘‡ , we then build a transfer
learning surrogate by combining all base surrogates:
ğ‘€TL = agg({ğ‘€1, ...,ğ‘€ğ¾,ğ‘€ğ‘‡ };w);

3) the surrogate ğ‘€TL is used to guide the configuration search,
instead of the original ğ‘€ğ‘‡ . Concretely, we use gPoE to combine
the multiple base surrogates (agg), and the parameters w are calculated
based on the ranking of configurations, which reflects the similarity
between the source tasks and the target task.


## Performance Rank

<p align="center">
<img src="https://raw.githubusercontent.com/thomas-young-2013/open-box/master/docs/imgs/tl_lightgbm_75_rank_result.svg" width="70%">
</p>

+ Average rank of tuning LightGBM with transfer learning
