# Parallel Evaluation

Most proposed Bayesian optimization (BO) approaches only allow the exploration of the search space to occur 
sequentially. To fully utilize computing resources in a parallel infrastructure, **OpenBox** provides a 
mechanism for distributed parallelization, where multiple configurations can be evaluated concurrently across workers. 

Two parallel settings are considered:

<p align="center">
<img src="https://raw.githubusercontent.com/thomas-young-2013/open-box/master/docs/imgs/parallel_bo.svg" width="90%">
</p>

1) **Synchronous parallel setting (left)**. The worker pulls a new configuration from the suggestion server to evaluate 
until all the workers have finished their last evaluations.

2) **Asynchronous parallel setting (right)**. The worker pulls a new configuration when the previous evaluation is completed.

**OpenBox** proposes a local penalization based parallelization mechanism, the goal of which is to sample new 
configurations that are promising and far enough from the configurations being evaluated by other workers.
This mechanism can handle the well-celebrated exploration vs. exploitation trade-off, and meanwhile prevent workers 
from exploring similar configurations.

In this tutorial, we illustrate how to optimize a problem in parallel manner on your local machine with **OpenBox**.

## Problem Setup

First, **define configuration space** to search and **define objective function**
to <font color=#FF0000>**minimize**</font>. Here we use the **Branin** function.

```python
import numpy as np
from openbox import sp

# Define Search Space
space = sp.Space()
x1 = sp.Real("x1", -5, 10, default_value=0)
x2 = sp.Real("x2", 0, 15, default_value=0)
space.add_variables([x1, x2])


# Define Objective Function
def branin(config):
    x1, x2 = config['x1'], config['x2']
    y = (x2 - 5.1 / (4 * np.pi ** 2) * x1 ** 2 + 5 / np.pi * x1 - 6) ** 2 \
        + 10 * (1 - 1 / (8 * np.pi)) * np.cos(x1) + 10
    return {'objs': (y,)}
```

If you are not familiar with the problem setup, please refer to [Quick Start Tutorial](../quick_start/quick_start).

## Parallel Evaluation on Local Machine

This time we use <font color=#FF0000>**ParallelOptimizer**</font> to optimize the objective function 
in a parallel manner on your local machine.

```python
from openbox import ParallelOptimizer

# Parallel Evaluation on Local Machine
opt = ParallelOptimizer(
    branin,
    space,
    parallel_strategy='async',
    batch_size=4,
    batch_strategy='median_imputation',
    num_objs=1,
    num_constraints=0,
    max_runs=50,
    surrogate_type='gp',
    time_limit_per_trial=180,
    task_id='parallel_sync',
)
history = opt.run()
```

In addition to **objective_function** and **space** being passed to **ParallelOptimizer**, 
the other parameters are as follows:

+ **parallel_strategy='async' / 'sync'** sets whether the parallel evaluation is performed asynchronously or synchronously.
We suggest using **'async'** because it makes better use of resources and achieves better performance than **'sync'**.

+ **batch_size=4** sets the number of parallel workers.

+ **batch_strategy='median_imputation'** sets the strategy on how to make multiple suggestions at the same time.
We suggest using **'median_imputation'** by default for stable performance.

+ **num_objs=1** and **num_constraints=0** indicates that our function returns a single objective value with no constraint. 

+ **max_runs=100** means the optimization will take 100 rounds (optimizing the objective function 100 times). 

+ **surrogate_type='gp'**. For mathematical problem, we suggest using Gaussian Process (**'gp'**) as Bayesian surrogate
model. For practical problems such as hyperparameter optimization (HPO), we suggest using Random Forest (**'prf'**).

+ **time_limit_per_trial** sets the time budget (seconds) of each objective function evaluation. Once the 
evaluation time exceeds this limit, objective function will return as a failed trial.

+ **task_id** is set to identify the optimization process.

After optimization, call <font color=#FF0000>**print(opt.get_history())**</font> to see the result:

```python
print(opt.get_history())
```

```
+----------------------------------------------+
| Parameters              | Optimal Value      |
+-------------------------+--------------------+
| x1                      | -3.138286          |
| x2                      | 12.292733          |
+-------------------------+--------------------+
| Optimal Objective Value | 0.3985991718620365 |
+-------------------------+--------------------+
| Num Configs             | 100                |
+-------------------------+--------------------+
```
