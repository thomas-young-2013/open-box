# 迁移学习


注意到在进行BBO时，用户经常运行与以前类似的任务。
这个观察为我们提供了加速BBO的方法。
与Vizier仅为单目标BBO问题提供有限的迁移学习功能相比，OpenBox采用了通用的迁移学习框架，具有以下优点：


1） 支持广义黑盒优化问题；
2） 与大多数贝叶斯优化方法兼容。

OpenBox 的输入是 𝐾 + 1 个任务： 𝐾 个之前的任务 D<sup>1</sup>, ...,
D<sup>𝐾</sup> 和一个当前的任务 D<sup>𝑇</sup>。
每个任务 D<sup>𝑖</sup> = {(𝒙, 𝒚)} ( 𝑖 = 1, ...,𝐾 )  包含一系列的观察。
注意到，𝒚 是一个包含多个目标配置 𝒙 的数组。
对于有 𝑝 个目标的多目标优化问题，我们提出单独迁移和这 𝑝 个目标相关的知识。
因此，多目标的迁移学习就被转化成了 𝑝 个单目标的迁移学习。
我们次啊用下述的迁移学习技术：

1）我们首先对于每个先验任务 𝐷<sup>𝑖</sup> （这里𝑖是第𝑖个先验）都训练一个替代模型𝑀<sup>𝑖</sup>，也对于 𝐷<sup>𝑇</sup> 训练一个模型 𝑀<sup>𝑇</sup>。

2）基于 𝑀<sup>1:𝐾</sup> 和 𝑀<sup>𝑇</sup> 我们通过结合所有的替代模型来构建一个迁移学习替代模型：𝑀<sup>TL</sup> = agg({𝑀<sup>1</sup>, ...,𝑀<sup>𝐾</sup>,𝑀<sup>𝑇</sup> };w)；

3）我们用替代模型 𝑀<sup>TL</sup> 来指导配置搜索，而不是用原始的模型 𝑀<sup>𝑇</sup>。

具体来说，我们使用gPoE来组合多个基本的代理模型（agg），这里参数w是根据构型排序计算出来的，它反映了源任务和目标任务之间的相似性。


## 性能对比
我们将OpenBox与主流的迁移学习基准方法Vizier和非迁移学习基准方法SMAC3进行对比。
每个算法的平均等级（越低越好）如下图所示。
有关实验设置、数据集信息和更多实验结果，请参阅我们的 [已发表文章]()。


<p align="center">
<img src="https://raw.githubusercontent.com/thomas-young-2013/open-box/master/docs/imgs/tl_lightgbm_75_rank_result.svg" width="70%">
</p>

+ 使用迁移学习调优LightGBM的平均等级
